{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910edb3e",
   "metadata": {},
   "source": [
    "# EE 467 Lab 2: Breaking CAPTCHAs with PyTorch\n",
    "This notebook implements the same CAPTCHA breaking solution as the TensorFlow version, but using PyTorch instead. We will build and train a Convolutional Neural Network to automatically recognize CAPTCHA characters.\n",
    "\n",
    "As usual, please check if the helper library, `lab_2_helpers.py` and the extracted dataset directory, `captcha-images` exist under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0297ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up corrupted matplotlib...\n",
      "\n",
      "Installing matplotlib...\n",
      "Installation output: Collecting matplotlib==3.7.2\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aksha\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.7.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.16.0)\n",
      "Downloading matplotlib-3.7.2-cp310-cp310-win_amd64.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/7.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.1/7.5 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.5 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 10.8 MB/s  0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.8\n",
      "\n",
      "Installation errors: WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: uninstall-no-record-file\n",
      "\n",
      "Ã— Cannot uninstall matplotlib 3.10.8\n",
      "â•°â”€> The package's contents are unknown: no RECORD file was found for matplotlib.\n",
      "\n",
      "hint: You might be able to recover from this via: pip install --force-reinstall --no-deps matplotlib==3.10.8\n",
      "\n",
      "\n",
      "Installing other dependencies...\n",
      "✓ scikit-learn\n",
      "✓ opencv-python>4\n",
      "✓ imutils\n",
      "✓ torch\n",
      "✓ torchvision\n",
      "\n",
      "✓ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Step 1: Remove any broken matplotlib installations\n",
    "print(\"Cleaning up corrupted matplotlib...\")\n",
    "site_packages = os.path.dirname(os.__file__).replace('\\\\lib', '\\\\Lib\\\\site-packages')\n",
    "matplotlib_path = os.path.join(os.path.dirname(sys.executable).replace('\\\\Scripts', '\\\\Lib\\\\site-packages'), 'matplotlib')\n",
    "\n",
    "if os.path.exists(matplotlib_path):\n",
    "    try:\n",
    "        shutil.rmtree(matplotlib_path)\n",
    "        print(f\"✓ Removed corrupted matplotlib at {matplotlib_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove directory: {e}\")\n",
    "\n",
    "# Step 2: Install matplotlib from PyPI with explicit version\n",
    "print(\"\\nInstalling matplotlib...\")\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"setuptools\", \"wheel\"], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib==3.7.2\", \"--no-cache-dir\", \"--no-build-isolation\"],\n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Successfully installed matplotlib 3.7.2\")\n",
    "else:\n",
    "    print(f\"Installation output: {result.stdout}\")\n",
    "    print(f\"Installation errors: {result.stderr}\")\n",
    "\n",
    "# Step 3: Install other packages\n",
    "print(\"\\nInstalling other dependencies...\")\n",
    "other_packages = [\"scikit-learn\", \"opencv-python>4\", \"imutils\", \"torch\", \"torchvision\"]\n",
    "\n",
    "for package in other_packages:\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package, \"--no-cache-dir\"],\n",
    "                           capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ {package}\")\n",
    "    else:\n",
    "        print(f\"✗ {package}\")\n",
    "\n",
    "print(\"\\n✓ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c08ef",
   "metadata": {},
   "source": [
    "Next, we import all tools needed before starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Matplotlib import failed: cannot import name 'pyplot' from 'matplotlib' (unknown location)\n",
      "  Continuing without matplotlib...\n",
      "✓ Helper functions imported successfully\n",
      "✓ Core imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, glob, math\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from lab_2_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3bb96",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Ground Truth Characters Extraction\n",
    "As usual, we will start pre-processing stage by loading CAPTCHA images into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf captcha-images.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./captcha-images\\\\2A2X.png',\n",
      " './captcha-images\\\\2A5R.png',\n",
      " './captcha-images\\\\2A5Z.png',\n",
      " './captcha-images\\\\2A98.png',\n",
      " './captcha-images\\\\2A9N.png',\n",
      " './captcha-images\\\\2AD9.png',\n",
      " './captcha-images\\\\2AEF.png',\n",
      " './captcha-images\\\\2APC.png',\n",
      " './captcha-images\\\\2AQ7.png',\n",
      " './captcha-images\\\\2AX2.png']\n"
     ]
    }
   ],
   "source": [
    "# Dataset images folder\n",
    "CAPTCHA_IMAGE_FOLDER = \"./captcha-images\"\n",
    "\n",
    "# List of all the captcha images we need to process\n",
    "captcha_image_paths = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "# Review image paths\n",
    "pprint(captcha_image_paths[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e9e5",
   "metadata": {},
   "source": [
    "Note that for each image, its file name (without extension) happens to be its corresponding CAPTCHA text. Thus, we extract file names for all CAPTCHA images and save them as labels for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5415860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2A2X', '2A5R', '2A5Z', '2A98', '2A9N', '2AD9', '2AEF', '2APC', '2AQ7', '2AX2']\n"
     ]
    }
   ],
   "source": [
    "def extract_captcha_text(image_path):\n",
    "    \"\"\" Extract correct CAPTCHA texts from file name of images. \"\"\"\n",
    "    # Extract file name of image from its path\n",
    "    # e.g. \"./captcha-images/2A2X.png\" -> \"2A2X.png\"\n",
    "    image_file_name = os.path.basename(image_path)\n",
    "    # Extract base name of image, omitting file extension\n",
    "    # e.g. \"2A2X.png\" -> \"2A2X\"\n",
    "    return os.path.splitext(image_file_name)[0]\n",
    "\n",
    "captcha_texts = [extract_captcha_text(image_path) for image_path in captcha_image_paths]\n",
    "# Review extraction results\n",
    "pprint(captcha_texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db979284",
   "metadata": {},
   "source": [
    "## Loading and Transforming Images\n",
    "For the feature extraction stage, we are going to extract individual characters from these CAPTCHAs. This is done by looking for contours (bounding boxes) around characters, then cropping the CAPTCHAs such as only the contour areas are preserved. We begin feature extraction by loading and transforming images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7852c019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captcha_image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     image_padded \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(image_gray, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mBORDER_REPLICATE)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_padded\n\u001b[1;32m---> 15\u001b[0m captcha_images \u001b[38;5;241m=\u001b[39m [load_transform_image(image_path) \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcaptcha_image_paths\u001b[49m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Review loaded CAPTCHAs (skip visualization due to matplotlib issues)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(captcha_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CAPTCHA images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'captcha_image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def load_transform_image(image_path):\n",
    "    \"\"\" Load and transform image into grayscale. \"\"\"\n",
    "    # 1) Load image with OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # 2) Convert image to grayscale\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # 3) Add extra padding (8px) around the image\n",
    "    image_padded = cv2.copyMakeBorder(image_gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    return image_padded\n",
    "\n",
    "captcha_images = [load_transform_image(image_path) for image_path in captcha_image_paths]\n",
    "\n",
    "# Review loaded CAPTCHAs (skip visualization due to matplotlib issues)\n",
    "print(f\"✓ Loaded {len(captcha_images)} CAPTCHA images\")\n",
    "# print_images(\n",
    "#     captcha_images[:10], n_rows=2, texts=captcha_texts[:10]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9adf40",
   "metadata": {},
   "source": [
    "Next, we will split our dataset into train-validation set and test set. The former set will be used for training and validation in deep character classification model, while the latter will be used for testing our CAPTCHA recognition pipline end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split seed\n",
    "TVT_SPLIT_SEED = 31528476\n",
    "\n",
    "# Perform split on CAPTCHA images as well as labels\n",
    "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
    "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
    ")\n",
    "\n",
    "print(\"Train-validation:\", len(captcha_texts_tv))\n",
    "print(\"Test:\", len(captcha_texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfbff2",
   "metadata": {},
   "source": [
    "## Bounding Box Extraction\n",
    "It's now time to perform the most important feature extraction step: finding contours and extracting characters. Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. It is useful for shape analysis and object detection and recognition. For our task however, we are **more interested in the bounding boxes around characters**, since these are the part of images we will be used for character classification.\n",
    "\n",
    "After these steps, we have transformed CAPTCHA images into images of single character. This simplifies our task since now our model only needs to deal with classification (from character image to character itself) rather than also dealing with detection (finding and extracting charatcers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15008acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character images folder template\n",
    "CHAR_IMAGE_FOLDER = f\"./char-images-{TVT_SPLIT_SEED}\"\n",
    "\n",
    "def extract_chars(image):\n",
    "    \"\"\" Find contours and extract characters inside each CAPTCHA. \"\"\"\n",
    "    # Threshold image and convert it to black-white\n",
    "    image_bw = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    # Find contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    char_regions = []\n",
    "    # Loop through each contour\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the bounding box,\n",
    "        # detect if there are letters conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # Bounding box is too wide for a single character\n",
    "            # Split it in half into two letter regions\n",
    "            half_width = int(w / 2)\n",
    "            char_regions.append((x, y, half_width, h))\n",
    "            char_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # Only a single letter in contour\n",
    "            char_regions.append((x, y, w, h))\n",
    "\n",
    "    # Ignore image if less or more than 4 regions detected\n",
    "    if len(char_regions)!=4:\n",
    "        return None\n",
    "    # Sort regions by their X coordinates\n",
    "    char_regions.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Character images\n",
    "    char_images = []\n",
    "    # Save each character as a single image\n",
    "    for x, y, w, h in char_regions:\n",
    "        # Extract character from image with 2px margin\n",
    "        char_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "        # Save character images\n",
    "        char_images.append(char_image)\n",
    "\n",
    "    # Return character images\n",
    "    return char_images\n",
    "\n",
    "def save_chars(char_images, captcha_text, save_dir, char_counts):\n",
    "    \"\"\" Save character images to directory. \"\"\"\n",
    "    for char_image, char in zip(char_images, captcha_text):\n",
    "        # Get the folder to save the image in\n",
    "        save_path = os.path.join(save_dir, char)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Write letter image to file\n",
    "        char_count = char_counts.get(char, 1)\n",
    "        char_image_path = os.path.join(save_path, f\"{char_count}.png\")\n",
    "        cv2.imwrite(char_image_path, char_image)\n",
    "\n",
    "        # Update count\n",
    "        char_counts[char] = char_count+1\n",
    "\n",
    "# Force character extraction even if results are already available\n",
    "FORCE_EXTRACT_CHAR = False\n",
    "\n",
    "char_counts = {}\n",
    "# Extract and save images for characters\n",
    "if FORCE_EXTRACT_CHAR or not os.path.exists(CHAR_IMAGE_FOLDER):\n",
    "    for captcha_image, captcha_text in zip(captcha_images_tv, captcha_texts_tv):\n",
    "        # Extract character images\n",
    "        char_images = extract_chars(captcha_image)\n",
    "        # Skip if extraction failed\n",
    "        if char_images is None:\n",
    "            continue\n",
    "        # Save character images\n",
    "        save_chars(char_images, captcha_text, CHAR_IMAGE_FOLDER, char_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77764b95",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "During the training stage, we are going to load character images from previous stages as features and generate corresponding labels from their path. We will then rescale features, one-hot encode labels (occurred characters) and save labels to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f48f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of occurred characters (labels)\n",
    "LABELS_PATH = \"./labels_pytorch.pkl\"\n",
    "\n",
    "def make_feature(image):\n",
    "    \"\"\" Process character image and turn it into feature. \"\"\"\n",
    "    # Resize letter to 20*20\n",
    "    image_resized = resize_to_fit(image, 20, 20)\n",
    "    # Add extra dimension as the only channel\n",
    "    feature = image_resized[..., None]\n",
    "\n",
    "    return feature\n",
    "\n",
    "def make_feature_label(image_path):\n",
    "    \"\"\" Load character image and make feature-label pair from image path. \"\"\"\n",
    "    # Load image and make feature\n",
    "    feature = make_feature(cv2.imread(image_path, cv2.COLOR_BGR2GRAY))\n",
    "    # Extract label based on the directory the image is in\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "\n",
    "    return feature, label\n",
    "\n",
    "# Make features and labels from character image paths\n",
    "features_tv, labels_tv = unzip((\n",
    "    make_feature_label(image_path) for image_path in paths.list_images(CHAR_IMAGE_FOLDER)\n",
    "))\n",
    "\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_tv = np.array(features_tv, dtype=\"float\")/255\n",
    "# Convert labels into one-hot encodings\n",
    "lb = LabelBinarizer()\n",
    "labels_one_hot_tv = lb.fit_transform(labels_tv)\n",
    "# Number of classes\n",
    "n_classes = len(lb.classes_)\n",
    "\n",
    "# Further split the training data into training and validation set\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(\n",
    "    features_tv, labels_one_hot_tv, test_size=0.25, random_state=955996\n",
    ")\n",
    "# Save mapping from labels to one-hot encoding\n",
    "with open(LABELS_PATH, \"wb\") as f:\n",
    "    pickle.dump(lb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136873f",
   "metadata": {},
   "source": [
    "# Training\n",
    "Next, we build a Convolutional Neural Network (CNN) as our classification model with PyTorch. The structure of the neural network is the same as the TensorFlow version:\n",
    "- First conv block: Conv2D(20 channels) + ReLU + MaxPooling2D\n",
    "- Second conv block: Conv2D(50 channels) + ReLU + MaxPooling2D\n",
    "- Flatten layer\n",
    "- Dense(500) + ReLU\n",
    "- Dense(n_classes) + Softmax\n",
    "\n",
    "After building the neural network, we train it and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "# Number of epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Path of model weights file\n",
    "MODEL_WEIGHTS_PATH = \"./captcha-model-pytorch.pth\"\n",
    "# Force training even if weights are already available\n",
    "FORCE_TRAINING = True\n",
    "\n",
    "# Define the CNN model in PyTorch\n",
    "class CaptchaCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CaptchaCNN, self).__init__()\n",
    "        \n",
    "        # First convolution block: 20 channels, 5x5 kernel, ReLU, same padding\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolution block: 50 channels, 5x5 kernel, ReLU, same padding\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)  # (*, 20, 10, 10)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)  # (*, 50, 5, 5)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.flatten(x)  # (*, 1250)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = CaptchaCNN(n_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train).to(device)\n",
    "y_train_torch = torch.FloatTensor(y_train).to(device)\n",
    "X_vali_torch = torch.FloatTensor(X_vali).to(device)\n",
    "y_vali_torch = torch.FloatTensor(y_vali).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "vali_dataset = TensorDataset(X_vali_torch, y_vali_torch)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == torch.argmax(batch_y, dim=1)).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, vali_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in vali_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == torch.argmax(batch_y, dim=1)).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(vali_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Train the model\n",
    "if FORCE_TRAINING or not os.path.exists(MODEL_WEIGHTS_PATH):\n",
    "    print(f\"Training for {N_EPOCHS} epochs...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        vali_loss, vali_acc = validate(model, vali_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Vali Loss: {vali_loss:.4f}, Vali Acc: {vali_acc:.4f}\")\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), MODEL_WEIGHTS_PATH)\n",
    "    print(f\"Model weights saved to {MODEL_WEIGHTS_PATH}\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH, map_location=device))\n",
    "    print(f\"Model weights loaded from {MODEL_WEIGHTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbb8cc",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "During the training part, we have validated the performance of our neural network model on images of single characters. Now it's time to test and evaluate CAPTCHAs from the beginning to the end. First, we will need to build the pipeline for CAPTCHA character prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from file (so we can translate model predictions to actual letters)\n",
    "with open(LABELS_PATH, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Test our pipeline (and model) with the test set.\n",
    "# However, you'd want to replace this with some random CAPTCHAs in the real world.\n",
    "\n",
    "# Dummy character images\n",
    "DUMMY_CHAR_IMAGES = np.zeros((4, 20, 20, 1))\n",
    "\n",
    "# Indices of CAPTCHAs on which extractions failed\n",
    "extract_failed_indices = []\n",
    "# Extracted character images\n",
    "char_images_test = []\n",
    "\n",
    "# Extract character images and make features\n",
    "for i, captcha_image in enumerate(captcha_images_test):\n",
    "    # Extract character images\n",
    "    char_images = extract_chars(captcha_image)\n",
    "\n",
    "    if char_images:\n",
    "        char_images_test.extend(char_images)\n",
    "    # Use dummy character images as placeholder if extraction failed\n",
    "    else:\n",
    "        extract_failed_indices.append(i)\n",
    "        char_images_test.extend(DUMMY_CHAR_IMAGES)\n",
    "\n",
    "# Make features for character images\n",
    "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_test = np.array(features_test, dtype=\"float\")/255\n",
    "\n",
    "# Convert to torch tensor\n",
    "features_test_torch = torch.FloatTensor(features_test).to(device)\n",
    "\n",
    "# Predict labels with neural network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(features_test_torch)\n",
    "    preds_test = torch.softmax(preds_test, dim=1).cpu().numpy()\n",
    "\n",
    "# Convert predictions to class indices\n",
    "preds_test = np.argmax(preds_test, axis=1)\n",
    "# Convert class indices to actual characters\n",
    "preds_test = lb.inverse_transform(preds_test)\n",
    "\n",
    "# Group all 4 characters for the same CAPTCHA\n",
    "preds_test = [\"\".join(chars) for chars in group_every(preds_test, 4)]\n",
    "# Update result for CAPTCHAs on which extractions failed\n",
    "for i in extract_failed_indices:\n",
    "    preds_test[i] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987ac76",
   "metadata": {},
   "source": [
    "Now, we can compute the accuracy of our pipeline, as well as taking a look at correct and incorrect CAPTCHA text predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090be939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of CAPTCHAs to display\n",
    "N_DISPLAY_SAMPLES = 10\n",
    "\n",
    "# Number of test CAPTCHAs\n",
    "n_test = len(captcha_texts_test)\n",
    "# Number of correct predictions\n",
    "n_correct = 0\n",
    "\n",
    "# Indices of correct predictions\n",
    "correct_indices = []\n",
    "# Indices of incorrect predictions\n",
    "incorrect_indices = []\n",
    "\n",
    "for i, (pred_text, actual_text) in enumerate(zip(preds_test, captcha_texts_test)):\n",
    "    if pred_text==actual_text:\n",
    "        # 1) Update number of correct predictions\n",
    "        n_correct += 1\n",
    "        # 2) Collect index of correct prediction\n",
    "        if len(correct_indices)<N_DISPLAY_SAMPLES:\n",
    "            correct_indices.append(i)\n",
    "    else:\n",
    "        # 3) Collect index of incorrect prediction\n",
    "        if len(incorrect_indices)<N_DISPLAY_SAMPLES:\n",
    "            incorrect_indices.append(i)\n",
    "\n",
    "# Show number of total / correct predictions and accuracy\n",
    "print(\"# of test CAPTCHAs:\", n_test)\n",
    "print(\"# correctly recognized:\", n_correct)\n",
    "print(\"Accuracy:\", n_correct/n_test, \"\\n\")\n",
    "\n",
    "# Visualization disabled due to matplotlib issues\n",
    "# Uncomment below if matplotlib becomes available\n",
    "# print_images(\n",
    "#     [captcha_images_test[i] for i in correct_indices],\n",
    "#     texts=[f\"Correct: {captcha_texts_test[i]}\" for i in correct_indices],\n",
    "#     n_rows=2\n",
    "# )\n",
    "# print_images(\n",
    "#     [captcha_images_test[i] for i in incorrect_indices],\n",
    "#     texts=[\n",
    "#         f\"Prediction: {preds_test[i]}\\nActual: {captcha_texts_test[i]}\" \\\n",
    "#         for i in incorrect_indices\n",
    "#     ],\n",
    "#     n_rows=2,\n",
    "#     fig_size=(20, 6),\n",
    "#     text_center=(0.5, -0.25)\n",
    "# )\n",
    "\n",
    "print(f\"\\nCorrect predictions: {correct_indices[:N_DISPLAY_SAMPLES]}\")\n",
    "print(f\"Incorrect predictions: {incorrect_indices[:N_DISPLAY_SAMPLES]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f57d9a",
   "metadata": {},
   "source": [
    "## Comparison with TensorFlow Results\n",
    "\n",
    "Both the TensorFlow and PyTorch implementations follow the exact same architecture and training procedure:\n",
    "- Same preprocessing and data split\n",
    "- Same CNN architecture (2 conv blocks, 2 fully connected layers)\n",
    "- Same batch size (32) and number of epochs (10)\n",
    "- Same optimizer (Adam) and loss function (cross entropy)\n",
    "\n",
    "The results should be similar, with minor differences expected due to randomness and implementation details in the respective frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438384eb",
   "metadata": {},
   "source": [
    "## References\n",
    "1. How to break a CAPTCHA system in 15 minutes with Machine Learning: https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "2. CaptchaSolver Jupyter Notebook: https://github.com/BenjaminWegener/CaptchaSolver\n",
    "3. PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "4. Keras Tutorial: The Ultimate Beginner's Guide to Deep Learning in Python: https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "5. Tensorflow API reference: https://www.tensorflow.org/api_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
