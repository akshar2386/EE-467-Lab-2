{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9525e05",
   "metadata": {},
   "source": [
    "# EE 467 Lab 2: Breaking CAPTCHAs with PyTorch\n",
    "\n",
    "This is the PyTorch implementation of the CAPTCHA breaking system. We will use PyTorch instead of TensorFlow/Keras to build and train the same CNN architecture for character classification.\n",
    "\n",
    "This notebook reuses the preprocessed data (character images and labels) from the TensorFlow version, but implements the model training and evaluation using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d15a7",
   "metadata": {},
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec51c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pprint import pprint\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from lab_2_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e84fd6",
   "metadata": {},
   "source": [
    "## Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144ad97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Constants\n",
    "CAPTCHA_IMAGE_FOLDER = \"./captcha-images\"\n",
    "TVT_SPLIT_SEED = 31528476\n",
    "CHAR_IMAGE_FOLDER = f\"./char-images-{TVT_SPLIT_SEED}\"\n",
    "LABELS_PATH = \"./labels.pkl\"\n",
    "MODEL_PATH = \"./captcha-model-pytorch.pt\"\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "FORCE_TRAINING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29e237",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "We reuse the character images and labels that were preprocessed in the TensorFlow notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5434775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of character classes: 32\n",
      "Classes: ['2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'J' 'K'\n",
      " 'L' 'M' 'N' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.5.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load labels (LabelBinarizer for converting between labels and one-hot encoding)\n",
    "with open(LABELS_PATH, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "n_classes = len(lb.classes_)\n",
    "print(f\"Number of character classes: {n_classes}\")\n",
    "print(f\"Classes: {lb.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff4d2e",
   "metadata": {},
   "source": [
    "## Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4380af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2685\n",
      "Validation set size: 895\n",
      "Feature shape: (20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "def make_feature(image):\n",
    "    \"\"\" Process character image and turn it into feature. \"\"\"\n",
    "    # Resize letter to 20*20\n",
    "    image_resized = resize_to_fit(image, 20, 20)\n",
    "    # Add extra dimension as the only channel\n",
    "    feature = image_resized[..., None]\n",
    "    return feature\n",
    "\n",
    "def make_feature_label(image_path):\n",
    "    \"\"\" Load character image and make feature-label pair from image path. \"\"\"\n",
    "    # Load image and make feature\n",
    "    feature = make_feature(cv2.imread(image_path, cv2.COLOR_BGR2GRAY))\n",
    "    # Extract label based on the directory the image is in\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    return feature, label\n",
    "\n",
    "# Make features and labels from character image paths\n",
    "features_tv, labels_tv = unzip((\n",
    "    make_feature_label(image_path) for image_path in paths.list_images(CHAR_IMAGE_FOLDER)\n",
    "))\n",
    "\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_tv = np.array(features_tv, dtype=\"float\")/255\n",
    "\n",
    "# Convert labels into one-hot encodings\n",
    "labels_one_hot_tv = lb.transform(labels_tv)\n",
    "\n",
    "# Further split the training data into training and validation set\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(\n",
    "    features_tv, labels_one_hot_tv, test_size=0.25, random_state=955996\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_vali)}\")\n",
    "print(f\"Feature shape: {X_train[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac608000",
   "metadata": {},
   "source": [
    "## Convert Data to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4be4cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loader batches: 84\n",
      "Validation loader batches: 28\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "# Move data to (N, C, H, W) format for PyTorch (currently N, H, W, C)\n",
    "X_train_tensor = torch.from_numpy(X_train).permute(0, 3, 1, 2).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "X_vali_tensor = torch.from_numpy(X_vali).permute(0, 3, 1, 2).float().to(device)\n",
    "y_vali_tensor = torch.from_numpy(y_vali).float().to(device)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "vali_dataset = TensorDataset(X_vali_tensor, y_vali_tensor)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training loader batches: {len(train_loader)}\")\n",
    "print(f\"Validation loader batches: {len(vali_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657024e5",
   "metadata": {},
   "source": [
    "## Define PyTorch CNN Model\n",
    "\n",
    "This CNN has the same architecture as the TensorFlow version:\n",
    "- Conv2D (20 filters, 5x5 kernel) + ReLU + MaxPool (2x2)\n",
    "- Conv2D (50 filters, 5x5 kernel) + ReLU + MaxPool (2x2)\n",
    "- Flatten\n",
    "- Dense (500 units) + ReLU\n",
    "- Dense (num_classes) + Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fae6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHACNNPyTorch(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1250, out_features=500, bias=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (fc2): Linear(in_features=500, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CAPTCHACNNPyTorch(nn.Module):\n",
    "    \"\"\"PyTorch CNN for CAPTCHA character classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(CAPTCHACNNPyTorch, self).__init__()\n",
    "        \n",
    "        # First convolution block: (*, 1, 20, 20) -> (*, 20, 20, 20) -> (*, 20, 10, 10)\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolution block: (*, 20, 10, 10) -> (*, 50, 10, 10) -> (*, 50, 5, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten: (*, 50, 5, 5) -> (*, 1250)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = CAPTCHACNNPyTorch(num_classes=n_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352abb2",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd98bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch [1/10] - Train Loss: 2.0415, Train Acc: 0.4607, Val Loss: 0.4966, Val Acc: 0.8816\n",
      "Epoch [2/10] - Train Loss: 0.1763, Train Acc: 0.9613, Val Loss: 0.1804, Val Acc: 0.9754\n",
      "Epoch [3/10] - Train Loss: 0.0748, Train Acc: 0.9821, Val Loss: 0.1711, Val Acc: 0.9754\n",
      "Epoch [4/10] - Train Loss: 0.0331, Train Acc: 0.9918, Val Loss: 0.1588, Val Acc: 0.9810\n",
      "Epoch [5/10] - Train Loss: 0.0145, Train Acc: 0.9970, Val Loss: 0.1561, Val Acc: 0.9788\n",
      "Epoch [6/10] - Train Loss: 0.0099, Train Acc: 0.9985, Val Loss: 0.1278, Val Acc: 0.9832\n",
      "Epoch [7/10] - Train Loss: 0.0046, Train Acc: 0.9993, Val Loss: 0.1430, Val Acc: 0.9844\n",
      "Epoch [8/10] - Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.1541, Val Acc: 0.9844\n",
      "Epoch [9/10] - Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.1600, Val Acc: 0.9844\n",
      "Epoch [10/10] - Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.1634, Val Acc: 0.9855\n",
      "\n",
      "Model saved to ./captcha-model-pytorch.pt\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, vali_loader, num_epochs, learning_rate, device):\n",
    "    \"\"\"Train the PyTorch model.\"\"\"\n",
    "    \n",
    "    # Loss function (CrossEntropyLoss is equivalent to categorical_crossentropy)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    vali_losses = []\n",
    "    train_accuracies = []\n",
    "    vali_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(batch_y.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        vali_loss = 0.0\n",
    "        vali_correct = 0\n",
    "        vali_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in vali_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                vali_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, labels = torch.max(batch_y.data, 1)\n",
    "                vali_total += labels.size(0)\n",
    "                vali_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        vali_loss /= len(vali_loader)\n",
    "        vali_accuracy = vali_correct / vali_total\n",
    "        vali_losses.append(vali_loss)\n",
    "        vali_accuracies.append(vali_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {vali_loss:.4f}, Val Acc: {vali_accuracy:.4f}')\n",
    "    \n",
    "    return train_losses, vali_losses, train_accuracies, vali_accuracies\n",
    "\n",
    "# Train the model\n",
    "if FORCE_TRAINING or not os.path.exists(MODEL_PATH):\n",
    "    print(\"Training the model...\")\n",
    "    train_losses, vali_losses, train_accs, vali_accs = train_model(\n",
    "        model, train_loader, vali_loader, N_EPOCHS, LEARNING_RATE, device\n",
    "    )\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    print(f\"\\nModel saved to {MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"Loading pretrained model...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba476346",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 228\n"
     ]
    }
   ],
   "source": [
    "# Load CAPTCHA images from the dataset\n",
    "from imutils import paths\n",
    "\n",
    "def extract_captcha_text(image_path):\n",
    "    \"\"\" Extract correct CAPTCHA texts from file name of images. \"\"\"\n",
    "    image_file_name = os.path.basename(image_path)\n",
    "    return os.path.splitext(image_file_name)[0]\n",
    "\n",
    "def load_transform_image(image_path):\n",
    "    \"\"\" Load and transform image into grayscale. \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_padded = cv2.copyMakeBorder(image_gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "    return image_padded\n",
    "\n",
    "# Load all CAPTCHA images\n",
    "captcha_image_paths = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "captcha_texts = [extract_captcha_text(image_path) for image_path in captcha_image_paths]\n",
    "captcha_images = [load_transform_image(image_path) for image_path in captcha_image_paths]\n",
    "\n",
    "# Split into train-validation and test sets\n",
    "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
    "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {len(captcha_texts_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e72c3f",
   "metadata": {},
   "source": [
    "## Helper Functions for Character Extraction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52347046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chars(image):\n",
    "    \"\"\" Find contours and extract characters inside each CAPTCHA. \"\"\"\n",
    "    # Threshold image and convert it to black-white\n",
    "    image_bw = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    # Find contours\n",
    "    contours = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    char_regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        if w / h > 1.25:\n",
    "            half_width = int(w / 2)\n",
    "            char_regions.append((x, y, half_width, h))\n",
    "            char_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            char_regions.append((x, y, w, h))\n",
    "\n",
    "    if len(char_regions) != 4:\n",
    "        return None\n",
    "    \n",
    "    char_regions.sort(key=lambda x: x[0])\n",
    "\n",
    "    char_images = []\n",
    "    for x, y, w, h in char_regions:\n",
    "        char_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "        char_images.append(char_image)\n",
    "\n",
    "    return char_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3734b",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed854ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Dummy character images\n",
    "DUMMY_CHAR_IMAGES = np.zeros((4, 20, 20, 1))\n",
    "\n",
    "# Indices of CAPTCHAs on which extractions failed\n",
    "extract_failed_indices = []\n",
    "# Extracted character images\n",
    "char_images_test = []\n",
    "\n",
    "# Extract character images\n",
    "for i, captcha_image in enumerate(captcha_images_test):\n",
    "    char_images = extract_chars(captcha_image)\n",
    "    if char_images:\n",
    "        char_images_test.extend(char_images)\n",
    "    else:\n",
    "        extract_failed_indices.append(i)\n",
    "        char_images_test.extend(DUMMY_CHAR_IMAGES)\n",
    "\n",
    "# Make features for character images\n",
    "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_test = np.array(features_test, dtype=\"float\")/255\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_test_tensor = torch.from_numpy(features_test).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "# Predict with PyTorch model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_logits = model(X_test_tensor)\n",
    "    # Convert logits to probabilities for LabelBinarizer\n",
    "    preds_probs = torch.softmax(preds_logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Convert predictions back to labels\n",
    "preds_test = lb.inverse_transform(preds_probs)\n",
    "\n",
    "# Group all 4 characters for the same CAPTCHA\n",
    "preds_test = [\" \".join(chars) for chars in group_every(preds_test, 4)]\n",
    "\n",
    "# Update result for CAPTCHAs on which extractions failed\n",
    "for i in extract_failed_indices:\n",
    "    preds_test[i] = \"-\"\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f847e",
   "metadata": {},
   "source": [
    "## Compute Accuracy and Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c585300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "# of test CAPTCHAs: 228\n",
      "# correctly recognized: 214\n",
      "Accuracy: 0.9386 (93.86%)\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of CAPTCHAs to display\n",
    "N_DISPLAY_SAMPLES = 10\n",
    "\n",
    "# Number of test CAPTCHAs\n",
    "n_test = len(captcha_texts_test)\n",
    "# Number of correct predictions\n",
    "n_correct = 0\n",
    "\n",
    "# Indices of correct predictions\n",
    "correct_indices = []\n",
    "# Indices of incorrect predictions\n",
    "incorrect_indices = []\n",
    "\n",
    "for i, (pred_text, actual_text) in enumerate(zip(preds_test, captcha_texts_test)):\n",
    "    # Remove spaces from prediction for comparison (due to formatting difference)\n",
    "    pred_text_clean = pred_text.replace(\" \", \"\")\n",
    "    \n",
    "    if pred_text_clean == actual_text:\n",
    "        n_correct += 1\n",
    "        if len(correct_indices) < N_DISPLAY_SAMPLES:\n",
    "            correct_indices.append(i)\n",
    "    else:\n",
    "        if len(incorrect_indices) < N_DISPLAY_SAMPLES:\n",
    "            incorrect_indices.append(i)\n",
    "\n",
    "# Show accuracy\n",
    "accuracy = n_correct / n_test if n_test > 0 else 0\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"# of test CAPTCHAs: {n_test}\")\n",
    "print(f\"# correctly recognized: {n_correct}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({100*accuracy:.2f}%)\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52347486",
   "metadata": {},
   "source": [
    "## Visualize Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a25e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Correct Predictions:\n",
      "--------------------------------------------------\n",
      "  Predicted: 8DQG\n",
      "  Predicted: 6TFC\n",
      "  Predicted: C2PW\n",
      "  Predicted: C2Z6\n",
      "  Predicted: 7WTS\n"
     ]
    }
   ],
   "source": [
    "# Show sample correct predictions\n",
    "if correct_indices:\n",
    "    print(\"Sample Correct Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in correct_indices[:5]:\n",
    "        print(f\"  Predicted: {captcha_texts_test[i]}\")\n",
    "else:\n",
    "    print(\"No correct predictions to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a952f",
   "metadata": {},
   "source": [
    "## Visualize Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "911ef13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Incorrect Predictions:\n",
      "--------------------------------------------------\n",
      "  Predicted: - | Actual: 5RSW\n",
      "  Predicted: - | Actual: C4SU\n",
      "  Predicted: - | Actual: 4J7H\n",
      "  Predicted: 2UMH | Actual: 2UW7\n",
      "  Predicted: - | Actual: K6MJ\n"
     ]
    }
   ],
   "source": [
    "# Show sample incorrect predictions\n",
    "if incorrect_indices:\n",
    "    print(\"Sample Incorrect Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in incorrect_indices[:5]:\n",
    "        pred_clean = preds_test[i].replace(\" \", \"\")\n",
    "        print(f\"  Predicted: {pred_clean} | Actual: {captcha_texts_test[i]}\")\n",
    "else:\n",
    "    print(\"No incorrect predictions to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5315f1",
   "metadata": {},
   "source": [
    "## Comparison Summary\n",
    "\n",
    "### Key Differences Between TensorFlow and PyTorch Implementations:\n",
    "\n",
    "1. **Framework Architecture**: \n",
    "   - TensorFlow uses the Keras Sequential API for high-level model definition\n",
    "   - PyTorch uses nn.Module for more explicit control\n",
    "\n",
    "2. **Model Definition**:\n",
    "   - TensorFlow: `model.add(layers.Conv2D(...))`\n",
    "   - PyTorch: `self.conv1 = nn.Conv2d(...)`\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - TensorFlow: Uses `model.fit()` for automatic training\n",
    "   - PyTorch: Manual training loop with forward/backward passes\n",
    "\n",
    "4. **Data Handling**:\n",
    "   - TensorFlow: Direct numpy array input\n",
    "   - PyTorch: DataLoader with TensorDataset for batching\n",
    "\n",
    "5. **Tensor Dimensions**:\n",
    "   - TensorFlow/Keras: Uses (N, H, W, C) format\n",
    "   - PyTorch: Uses (N, C, H, W) format (requires permutation)\n",
    "\n",
    "### Results:\n",
    "\n",
    "Both implementations use the same CNN architecture and should produce **similar accuracy results** on the test set. Minor differences may occur due to:\n",
    "- Different random initialization schemes\n",
    "- Different default optimizer behaviors\n",
    "- Floating-point precision differences\n",
    "\n",
    "The PyTorch implementation successfully demonstrates that the CAPTCHA breaking task is not framework-dependent - the model architecture and training approach are what matter most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910edb3e",
   "metadata": {},
   "source": [
    "# EE 467 Lab 2: Breaking CAPTCHAs with PyTorch\n",
    "This notebook implements the same CAPTCHA breaking solution as the TensorFlow version, but using PyTorch instead. We will build and train a Convolutional Neural Network to automatically recognize CAPTCHA characters.\n",
    "\n",
    "As usual, please check if the helper library, `lab_2_helpers.py` and the extracted dataset directory, `captcha-images` exist under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0297ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up corrupted matplotlib...\n",
      "\n",
      "Installing matplotlib...\n",
      "Installation output: Collecting matplotlib==3.7.2\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aksha\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.7.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.16.0)\n",
      "Downloading matplotlib-3.7.2-cp310-cp310-win_amd64.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.4/7.5 MB 15.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.0/7.5 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 16.6 MB/s  0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.8\n",
      "\n",
      "Installation errors: WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\aksha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: uninstall-no-record-file\n",
      "\n",
      "Ã— Cannot uninstall matplotlib 3.10.8\n",
      "â•°â”€> The package's contents are unknown: no RECORD file was found for matplotlib.\n",
      "\n",
      "hint: You might be able to recover from this via: pip install --force-reinstall --no-deps matplotlib==3.10.8\n",
      "\n",
      "\n",
      "Installing other dependencies...\n",
      "✓ scikit-learn\n",
      "✓ opencv-python>4\n",
      "✓ imutils\n",
      "✓ torch\n",
      "✓ torchvision\n",
      "\n",
      "✓ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade \"numpy<2\" \"tensorflow>=2\"\n",
    "%pip install \"opencv-python>4\" imutils torch torchvision scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c08ef",
   "metadata": {},
   "source": [
    "Next, we import all tools needed before starting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0625235a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pyplot' from 'matplotlib' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimutils\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m paths\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSpec\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pyplot' from 'matplotlib' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os, pickle, glob, math\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from imutils import paths\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from lab_2_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3bb96",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Ground Truth Characters Extraction\n",
    "As usual, we will start pre-processing stage by loading CAPTCHA images into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xJf captcha-images.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./captcha-images\\\\2A2X.png',\n",
      " './captcha-images\\\\2A5R.png',\n",
      " './captcha-images\\\\2A5Z.png',\n",
      " './captcha-images\\\\2A98.png',\n",
      " './captcha-images\\\\2A9N.png',\n",
      " './captcha-images\\\\2AD9.png',\n",
      " './captcha-images\\\\2AEF.png',\n",
      " './captcha-images\\\\2APC.png',\n",
      " './captcha-images\\\\2AQ7.png',\n",
      " './captcha-images\\\\2AX2.png']\n"
     ]
    }
   ],
   "source": [
    "# Dataset images folder\n",
    "CAPTCHA_IMAGE_FOLDER = \"./captcha-images\"\n",
    "\n",
    "# List of all the captcha images we need to process\n",
    "captcha_image_paths = list(paths.list_images(CAPTCHA_IMAGE_FOLDER))\n",
    "# Review image paths\n",
    "pprint(captcha_image_paths[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e9e5",
   "metadata": {},
   "source": [
    "Note that for each image, its file name (without extension) happens to be its corresponding CAPTCHA text. Thus, we extract file names for all CAPTCHA images and save them as labels for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5415860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2A2X', '2A5R', '2A5Z', '2A98', '2A9N', '2AD9', '2AEF', '2APC', '2AQ7', '2AX2']\n"
     ]
    }
   ],
   "source": [
    "def extract_captcha_text(image_path):\n",
    "    \"\"\" Extract correct CAPTCHA texts from file name of images. \"\"\"\n",
    "    # Extract file name of image from its path\n",
    "    # e.g. \"./captcha-images/2A2X.png\" -> \"2A2X.png\"\n",
    "    image_file_name = os.path.basename(image_path)\n",
    "    # Extract base name of image, omitting file extension\n",
    "    # e.g. \"2A2X.png\" -> \"2A2X\"\n",
    "    return os.path.splitext(image_file_name)[0]\n",
    "\n",
    "captcha_texts = [extract_captcha_text(image_path) for image_path in captcha_image_paths]\n",
    "# Review extraction results\n",
    "pprint(captcha_texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db979284",
   "metadata": {},
   "source": [
    "## Loading and Transforming Images\n",
    "For the feature extraction stage, we are going to extract individual characters from these CAPTCHAs. This is done by looking for contours (bounding boxes) around characters, then cropping the CAPTCHAs such as only the contour areas are preserved. We begin feature extraction by loading and transforming images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852c019",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captcha_image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     image_padded \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(image_gray, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mBORDER_REPLICATE)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_padded\n\u001b[1;32m---> 15\u001b[0m captcha_images \u001b[38;5;241m=\u001b[39m [load_transform_image(image_path) \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcaptcha_image_paths\u001b[49m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Review loaded CAPTCHAs (skip visualization due to matplotlib issues)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(captcha_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CAPTCHA images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'captcha_image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def load_transform_image(image_path):\n",
    "    \"\"\" Load and transform image into grayscale. \"\"\"\n",
    "    # 1) Load image with OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # 2) Convert image to grayscale\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # 3) Add extra padding (8px) around the image\n",
    "    image_padded = cv2.copyMakeBorder(image_gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    return image_padded\n",
    "\n",
    "captcha_images = [load_transform_image(image_path) for image_path in captcha_image_paths]\n",
    "\n",
    "# Review loaded CAPTCHAs (skip visualization due to matplotlib issues)\n",
    "print(f\"✓ Loaded {len(captcha_images)} CAPTCHA images\")\n",
    "# print_images(\n",
    "#     captcha_images[:10], n_rows=2, texts=captcha_texts[:10]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9adf40",
   "metadata": {},
   "source": [
    "Next, we will split our dataset into train-validation set and test set. The former set will be used for training and validation in deep character classification model, while the latter will be used for testing our CAPTCHA recognition pipline end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split seed\n",
    "TVT_SPLIT_SEED = 31528476\n",
    "\n",
    "# Perform split on CAPTCHA images as well as labels\n",
    "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
    "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
    ")\n",
    "\n",
    "print(\"Train-validation:\", len(captcha_texts_tv))\n",
    "print(\"Test:\", len(captcha_texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfbff2",
   "metadata": {},
   "source": [
    "## Bounding Box Extraction\n",
    "It's now time to perform the most important feature extraction step: finding contours and extracting characters. Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. It is useful for shape analysis and object detection and recognition. For our task however, we are **more interested in the bounding boxes around characters**, since these are the part of images we will be used for character classification.\n",
    "\n",
    "After these steps, we have transformed CAPTCHA images into images of single character. This simplifies our task since now our model only needs to deal with classification (from character image to character itself) rather than also dealing with detection (finding and extracting charatcers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15008acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character images folder template\n",
    "CHAR_IMAGE_FOLDER = f\"./char-images-{TVT_SPLIT_SEED}\"\n",
    "\n",
    "def extract_chars(image):\n",
    "    \"\"\" Find contours and extract characters inside each CAPTCHA. \"\"\"\n",
    "    # Threshold image and convert it to black-white\n",
    "    image_bw = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "    # Find contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    char_regions = []\n",
    "    # Loop through each contour\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Compare the width and height of the bounding box,\n",
    "        # detect if there are letters conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # Bounding box is too wide for a single character\n",
    "            # Split it in half into two letter regions\n",
    "            half_width = int(w / 2)\n",
    "            char_regions.append((x, y, half_width, h))\n",
    "            char_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # Only a single letter in contour\n",
    "            char_regions.append((x, y, w, h))\n",
    "\n",
    "    # Ignore image if less or more than 4 regions detected\n",
    "    if len(char_regions)!=4:\n",
    "        return None\n",
    "    # Sort regions by their X coordinates\n",
    "    char_regions.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Character images\n",
    "    char_images = []\n",
    "    # Save each character as a single image\n",
    "    for x, y, w, h in char_regions:\n",
    "        # Extract character from image with 2px margin\n",
    "        char_image = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "        # Save character images\n",
    "        char_images.append(char_image)\n",
    "\n",
    "    # Return character images\n",
    "    return char_images\n",
    "\n",
    "def save_chars(char_images, captcha_text, save_dir, char_counts):\n",
    "    \"\"\" Save character images to directory. \"\"\"\n",
    "    for char_image, char in zip(char_images, captcha_text):\n",
    "        # Get the folder to save the image in\n",
    "        save_path = os.path.join(save_dir, char)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Write letter image to file\n",
    "        char_count = char_counts.get(char, 1)\n",
    "        char_image_path = os.path.join(save_path, f\"{char_count}.png\")\n",
    "        cv2.imwrite(char_image_path, char_image)\n",
    "\n",
    "        # Update count\n",
    "        char_counts[char] = char_count+1\n",
    "\n",
    "# Force character extraction even if results are already available\n",
    "FORCE_EXTRACT_CHAR = False\n",
    "\n",
    "char_counts = {}\n",
    "# Extract and save images for characters\n",
    "if FORCE_EXTRACT_CHAR or not os.path.exists(CHAR_IMAGE_FOLDER):\n",
    "    for captcha_image, captcha_text in zip(captcha_images_tv, captcha_texts_tv):\n",
    "        # Extract character images\n",
    "        char_images = extract_chars(captcha_image)\n",
    "        # Skip if extraction failed\n",
    "        if char_images is None:\n",
    "            continue\n",
    "        # Save character images\n",
    "        save_chars(char_images, captcha_text, CHAR_IMAGE_FOLDER, char_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77764b95",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "During the training stage, we are going to load character images from previous stages as features and generate corresponding labels from their path. We will then rescale features, one-hot encode labels (occurred characters) and save labels to an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f48f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of occurred characters (labels)\n",
    "LABELS_PATH = \"./labels_pytorch.pkl\"\n",
    "\n",
    "def make_feature(image):\n",
    "    \"\"\" Process character image and turn it into feature. \"\"\"\n",
    "    # Resize letter to 20*20\n",
    "    image_resized = resize_to_fit(image, 20, 20)\n",
    "    # Add extra dimension as the only channel\n",
    "    feature = image_resized[..., None]\n",
    "\n",
    "    return feature\n",
    "\n",
    "def make_feature_label(image_path):\n",
    "    \"\"\" Load character image and make feature-label pair from image path. \"\"\"\n",
    "    # Load image and make feature\n",
    "    feature = make_feature(cv2.imread(image_path, cv2.COLOR_BGR2GRAY))\n",
    "    # Extract label based on the directory the image is in\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "\n",
    "    return feature, label\n",
    "\n",
    "# Make features and labels from character image paths\n",
    "features_tv, labels_tv = unzip((\n",
    "    make_feature_label(image_path) for image_path in paths.list_images(CHAR_IMAGE_FOLDER)\n",
    "))\n",
    "\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_tv = np.array(features_tv, dtype=\"float\")/255\n",
    "# Convert labels into one-hot encodings\n",
    "lb = LabelBinarizer()\n",
    "labels_one_hot_tv = lb.fit_transform(labels_tv)\n",
    "# Number of classes\n",
    "n_classes = len(lb.classes_)\n",
    "\n",
    "# Further split the training data into training and validation set\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(\n",
    "    features_tv, labels_one_hot_tv, test_size=0.25, random_state=955996\n",
    ")\n",
    "# Save mapping from labels to one-hot encoding\n",
    "with open(LABELS_PATH, \"wb\") as f:\n",
    "    pickle.dump(lb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136873f",
   "metadata": {},
   "source": [
    "# Training\n",
    "Next, we build a Convolutional Neural Network (CNN) as our classification model with PyTorch. The structure of the neural network is the same as the TensorFlow version:\n",
    "- First conv block: Conv2D(20 channels) + ReLU + MaxPooling2D\n",
    "- Second conv block: Conv2D(50 channels) + ReLU + MaxPooling2D\n",
    "- Flatten layer\n",
    "- Dense(500) + ReLU\n",
    "- Dense(n_classes) + Softmax\n",
    "\n",
    "After building the neural network, we train it and save weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "# Number of epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Path of model weights file\n",
    "MODEL_WEIGHTS_PATH = \"./captcha-model-pytorch.pth\"\n",
    "# Force training even if weights are already available\n",
    "FORCE_TRAINING = True\n",
    "\n",
    "# Define the CNN model in PyTorch\n",
    "class CaptchaCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CaptchaCNN, self).__init__()\n",
    "        \n",
    "        # First convolution block: 20 channels, 5x5 kernel, ReLU, same padding\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolution block: 50 channels, 5x5 kernel, ReLU, same padding\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(50 * 5 * 5, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)  # (*, 20, 10, 10)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)  # (*, 50, 5, 5)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.flatten(x)  # (*, 1250)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = CaptchaCNN(n_classes).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train).to(device)\n",
    "y_train_torch = torch.FloatTensor(y_train).to(device)\n",
    "X_vali_torch = torch.FloatTensor(X_vali).to(device)\n",
    "y_vali_torch = torch.FloatTensor(y_vali).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "vali_dataset = TensorDataset(X_vali_torch, y_vali_torch)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == torch.argmax(batch_y, dim=1)).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, vali_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in vali_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == torch.argmax(batch_y, dim=1)).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(vali_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Train the model\n",
    "if FORCE_TRAINING or not os.path.exists(MODEL_WEIGHTS_PATH):\n",
    "    print(f\"Training for {N_EPOCHS} epochs...\")\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        vali_loss, vali_acc = validate(model, vali_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Vali Loss: {vali_loss:.4f}, Vali Acc: {vali_acc:.4f}\")\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), MODEL_WEIGHTS_PATH)\n",
    "    print(f\"Model weights saved to {MODEL_WEIGHTS_PATH}\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS_PATH, map_location=device))\n",
    "    print(f\"Model weights loaded from {MODEL_WEIGHTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbb8cc",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "During the training part, we have validated the performance of our neural network model on images of single characters. Now it's time to test and evaluate CAPTCHAs from the beginning to the end. First, we will need to build the pipeline for CAPTCHA character prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels from file (so we can translate model predictions to actual letters)\n",
    "with open(LABELS_PATH, \"rb\") as f:\n",
    "    lb = pickle.load(f)\n",
    "\n",
    "# Test our pipeline (and model) with the test set.\n",
    "# However, you'd want to replace this with some random CAPTCHAs in the real world.\n",
    "\n",
    "# Dummy character images\n",
    "DUMMY_CHAR_IMAGES = np.zeros((4, 20, 20, 1))\n",
    "\n",
    "# Indices of CAPTCHAs on which extractions failed\n",
    "extract_failed_indices = []\n",
    "# Extracted character images\n",
    "char_images_test = []\n",
    "\n",
    "# Extract character images and make features\n",
    "for i, captcha_image in enumerate(captcha_images_test):\n",
    "    # Extract character images\n",
    "    char_images = extract_chars(captcha_image)\n",
    "\n",
    "    if char_images:\n",
    "        char_images_test.extend(char_images)\n",
    "    # Use dummy character images as placeholder if extraction failed\n",
    "    else:\n",
    "        extract_failed_indices.append(i)\n",
    "        char_images_test.extend(DUMMY_CHAR_IMAGES)\n",
    "\n",
    "# Make features for character images\n",
    "features_test = [make_feature(char_image) for char_image in char_images_test]\n",
    "# Scale raw pixel values into range [0, 1]\n",
    "features_test = np.array(features_test, dtype=\"float\")/255\n",
    "\n",
    "# Convert to torch tensor\n",
    "features_test_torch = torch.FloatTensor(features_test).to(device)\n",
    "\n",
    "# Predict labels with neural network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(features_test_torch)\n",
    "    preds_test = torch.softmax(preds_test, dim=1).cpu().numpy()\n",
    "\n",
    "# Convert predictions to class indices\n",
    "preds_test = np.argmax(preds_test, axis=1)\n",
    "# Convert class indices to actual characters\n",
    "preds_test = lb.inverse_transform(preds_test)\n",
    "\n",
    "# Group all 4 characters for the same CAPTCHA\n",
    "preds_test = [\"\".join(chars) for chars in group_every(preds_test, 4)]\n",
    "# Update result for CAPTCHAs on which extractions failed\n",
    "for i in extract_failed_indices:\n",
    "    preds_test[i] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987ac76",
   "metadata": {},
   "source": [
    "Now, we can compute the accuracy of our pipeline, as well as taking a look at correct and incorrect CAPTCHA text predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090be939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of CAPTCHAs to display\n",
    "N_DISPLAY_SAMPLES = 10\n",
    "\n",
    "# Number of test CAPTCHAs\n",
    "n_test = len(captcha_texts_test)\n",
    "# Number of correct predictions\n",
    "n_correct = 0\n",
    "\n",
    "# Indices of correct predictions\n",
    "correct_indices = []\n",
    "# Indices of incorrect predictions\n",
    "incorrect_indices = []\n",
    "\n",
    "for i, (pred_text, actual_text) in enumerate(zip(preds_test, captcha_texts_test)):\n",
    "    if pred_text==actual_text:\n",
    "        # 1) Update number of correct predictions\n",
    "        n_correct += 1\n",
    "        # 2) Collect index of correct prediction\n",
    "        if len(correct_indices)<N_DISPLAY_SAMPLES:\n",
    "            correct_indices.append(i)\n",
    "    else:\n",
    "        # 3) Collect index of incorrect prediction\n",
    "        if len(incorrect_indices)<N_DISPLAY_SAMPLES:\n",
    "            incorrect_indices.append(i)\n",
    "\n",
    "# Show number of total / correct predictions and accuracy\n",
    "print(\"# of test CAPTCHAs:\", n_test)\n",
    "print(\"# correctly recognized:\", n_correct)\n",
    "print(\"Accuracy:\", n_correct/n_test, \"\\n\")\n",
    "\n",
    "# Visualization disabled due to matplotlib issues\n",
    "# Uncomment below if matplotlib becomes available\n",
    "# print_images(\n",
    "#     [captcha_images_test[i] for i in correct_indices],\n",
    "#     texts=[f\"Correct: {captcha_texts_test[i]}\" for i in correct_indices],\n",
    "#     n_rows=2\n",
    "# )\n",
    "# print_images(\n",
    "#     [captcha_images_test[i] for i in incorrect_indices],\n",
    "#     texts=[\n",
    "#         f\"Prediction: {preds_test[i]}\\nActual: {captcha_texts_test[i]}\" \\\n",
    "#         for i in incorrect_indices\n",
    "#     ],\n",
    "#     n_rows=2,\n",
    "#     fig_size=(20, 6),\n",
    "#     text_center=(0.5, -0.25)\n",
    "# )\n",
    "\n",
    "print(f\"\\nCorrect predictions: {correct_indices[:N_DISPLAY_SAMPLES]}\")\n",
    "print(f\"Incorrect predictions: {incorrect_indices[:N_DISPLAY_SAMPLES]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f57d9a",
   "metadata": {},
   "source": [
    "## Comparison with TensorFlow Results\n",
    "\n",
    "Both the TensorFlow and PyTorch implementations follow the exact same architecture and training procedure:\n",
    "- Same preprocessing and data split\n",
    "- Same CNN architecture (2 conv blocks, 2 fully connected layers)\n",
    "- Same batch size (32) and number of epochs (10)\n",
    "- Same optimizer (Adam) and loss function (cross entropy)\n",
    "\n",
    "The results should be similar, with minor differences expected due to randomness and implementation details in the respective frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438384eb",
   "metadata": {},
   "source": [
    "## References\n",
    "1. How to break a CAPTCHA system in 15 minutes with Machine Learning: https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "2. CaptchaSolver Jupyter Notebook: https://github.com/BenjaminWegener/CaptchaSolver\n",
    "3. PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "4. Keras Tutorial: The Ultimate Beginner's Guide to Deep Learning in Python: https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "5. Tensorflow API reference: https://www.tensorflow.org/api_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
